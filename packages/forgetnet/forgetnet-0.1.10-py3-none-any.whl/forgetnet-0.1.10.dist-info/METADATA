Metadata-Version: 2.1
Name: forgetnet
Version: 0.1.10
Summary: A package for applying differential privacy to model training using block-level gradient shuffling
Home-page: https://github.com/dzagardo/forgetnet
Author: David Zagardo
Author-email: dave@greenwillowstudios.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy
Requires-Dist: torch
Requires-Dist: trl

# ğŸ›¡ï¸ ForgetNet: Differentially Private Block-wise Gradient Shuffle for Deep Learning ğŸ§ 

[![PyPI version](https://badge.fury.io/py/forgetnet.svg)](https://badge.fury.io/py/forgetnet)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

ForgetNet introduces a novel privacy-preserving technique for deep learning: Differentially Private Block-wise Gradient Shuffle (DP-BloGS). ğŸ”’ğŸ”€

## ğŸŒŸ Features

- ğŸš€ Fast training times, close to non-private training
- ğŸ¯ Competitive privacy guarantees compared to DP-SGD
- ğŸ“Š Better privacy-utility trade-off in many scenarios
- ğŸ‹ï¸ Scalable to large models (tested up to 1.1 billion parameters)
- ğŸ§® Parameter-wise privacy budget allocation

## ğŸ“¦ Installation

```
pip install forgetnet
```

## ğŸš€ Quick Start

```python
from forgetnet import BloGSSFTTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load your model and tokenizer
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Initialize the DP-BloGS trainer
trainer = BloGSSFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    dataset_text_field="text",
    target_epsilon=1.0,
    delta=1e-5,
    clip_value=1.0
)

# Train your model with privacy guarantees
trainer.train()
```

## ğŸ“š How It Works

DP-BloGS introduces a probabilistic approach to gradient noise through block-wise shuffling:

1. ğŸ“Š Divide gradients into blocks
2. ğŸ”€ Shuffle blocks randomly
3. ğŸ“ Apply parameter-specific block sizes
4. âœ‚ï¸ Use batch layer clipping
5. ğŸ§® Accumulate gradients

This combination allows for fast training while maintaining strong privacy guarantees!

## ğŸ“ˆ Performance

DP-BloGS has been tested on various model architectures, including:

- GPT-2 (124M)
- BERT (110M)
- OPT (350M)
- BLOOM (560M)
- TinyLlama (1.1B)

Results show competitive or better performance compared to DP-SGD in terms of:

- ğŸƒâ€â™‚ï¸ Training speed
- ğŸ­ Privacy guarantees
- ğŸ“Š Model utility

## ğŸ“„ Citation

If you use ForgetNet in your research, please cite my paper:

```bibtex
@article{zagardo2024dpblogs,
  title={Differentially Private Block-wise Gradient Shuffle for Deep Learning},
  author={Zagardo, David},
  journal={arXiv preprint arXiv:2024.XXXXX},
  year={2024}
}
```

## ğŸ¤ Contributing

We welcome contributions! Please see my [CONTRIBUTING.md](CONTRIBUTING.md) for details on how to get started.

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgements

We thank the open-source community and the authors of the papers cited in our work for their valuable contributions to the field of privacy-preserving machine learning.

---

Built with ğŸ§  by David Zagardo
