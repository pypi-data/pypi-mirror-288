#####################################################.
#     This file stores functions from REPORT        #
#####################################################.

import os
import sys
import glob
import pandas as pd
import textwrap
from pathlib import Path


def get_csv_names(self,command_line):
    """
    Detects the options from a command line or add them from manual inputs
    """
    
    csv_name = ''
    if '--csv_name' in command_line:
        csv_name = command_line.split('--csv_name')[1].split()[0]
        if csv_name[0] in ['"',"'"]:
            csv_name = csv_name[1:]
        if csv_name[-1] in ['"',"'"]:
            csv_name = csv_name[:-1]
    
    csv_test = ''
    if '--csv_test' in command_line:
        csv_test = command_line.split('--csv_test')[1].split()[0]
        if csv_test[0] in ['"',"'"]:
            csv_test = csv_test[1:]
        if csv_test[-1] in ['"',"'"]:
            csv_test = csv_test[:-1]

    if self.args.csv_name == '':
        self.args.csv_name = csv_name

    if self.args.csv_test == '':
        self.args.csv_test = csv_test

    return self


def get_images(module,file=None,pred_type='reg'):
    """
    Generate the string that includes images
    """
    
    csv_test_exists = False
    module_path = Path(f'{os.getcwd()}/{module}')
    if module.upper() == 'GENERATE':
        module_images = glob.glob(f'{module_path}/Raw_data/*.png')
    else:
        module_images = glob.glob(f'{module_path}/*.png')

    if module not in ['PREDICT','VERIFY']:
        image_caption = f'<br>------- Images generated by the {module} module -------'
    else:
        image_caption = f'<br>------- Images and summary generated by the {module} module -------'
        module_file = f'{os.getcwd()}/{module}/{module}_data.dat'
        columns_score = []
        for suffix in ['No PFI','PFI']:
            columns_score.append(get_summary(module,module_file,suffix,titles=True,pred_type=pred_type))
        
        # Combine both columns
        image_caption += combine_cols(columns_score)

    if module == 'VERIFY':
        if len(module_images) == 2 and 'No_PFI' in module_images[1]:
            module_images = revert_list(module_images)

    if module == 'PREDICT':
        results_images = []
        results_train, results_valid, results_test = [],[],[]
        shap_images = []
        pfi_images = []
        outliers_images = []
        variability_images = []
        csv_test_images_results = []
        csv_test_images_variability = []

        for image_path in module_images:
            filename = Path(image_path).stem
            if "_REPORT" not in filename:
                if "Results_" in filename:
                    results_images.append(image_path)
                elif "Variability_" in filename:
                    variability_images.append(image_path)
                elif "SHAP_" in filename:
                    shap_images.append(image_path)
                elif "Outliers_" in filename:
                    outliers_images.append(image_path)
                elif "PFI_" in filename:
                    pfi_images.append(image_path)

        if pred_type == 'clas':
            for image_path in module_images:
                if '_train.png' in image_path:
                    results_train.append(image_path)
                if '_valid.png' in image_path:
                    results_valid.append(image_path)
                if '_test.png' in image_path:
                    results_test.append(image_path)

        if os.path.exists(f'{module_path}/csv_test'):
            csv_test_images = glob.glob(f'{module_path}/csv_test/*.png')
            for image_path in csv_test_images:
                filename = Path(image_path).stem
                if "Results_" in filename:
                    csv_test_images_results.append(image_path)
                elif "Variability_" in filename:
                    csv_test_images_variability.append(image_path)

        # keep the ordering (No_PFI in the left, PFI in the right of the PDF)
        if len(results_images) == 2 and 'No_PFI' in results_images[1]:
            results_images = revert_list(results_images)
        if len(results_train) == 2 and 'No_PFI' in results_train[1]:
            results_train = revert_list(results_train)
        if len(results_valid) == 2 and 'No_PFI' in results_valid[1]:
            results_valid = revert_list(results_valid)
        if len(results_test) == 2 and 'No_PFI' in results_test[1]:
            results_test = revert_list(results_test)
        if len(variability_images) == 2 and 'No_PFI' in variability_images[1]:
            variability_images = revert_list(variability_images)
        if len(shap_images) == 2 and 'No_PFI' in shap_images[1]:
            shap_images = revert_list(shap_images)
        if len(pfi_images) == 2 and 'No_PFI' in pfi_images[1]:
            pfi_images = revert_list(pfi_images)
        if len(outliers_images) == 2 and 'No_PFI' in outliers_images[1]:
            outliers_images = revert_list(outliers_images)
        if len(csv_test_images_results) == 2 and 'No_PFI' in csv_test_images_results[1]:
            csv_test_images_results = revert_list(csv_test_images_results)
        if len(csv_test_images_variability) == 2 and 'No_PFI' in csv_test_images_variability[1]:
            csv_test_images_variability = revert_list(csv_test_images_variability)

        if pred_type == 'reg':
            image_pair_list = [results_images, variability_images, shap_images, pfi_images, outliers_images,csv_test_images_results]
            image_pair_list_test = [csv_test_images_variability]
        elif pred_type == 'clas':
            image_pair_list = [results_train, results_valid, results_test, shap_images, pfi_images, outliers_images]
            image_pair_list_test = [csv_test_images_results]

        html_png = ''
        for _,image_pair in enumerate(image_pair_list):
            pair_list = ''.join([f'<img src="file:///{image_path}" style="margin-bottom: 10px; margin-top: 10px; margin-left: -13px; width: 100%, margin: 0"/>' for image_path in image_pair])
            html_png += f'<pre style="text-align: center;">{pair_list}</pre>'
        
        # summary of the external CSV test set (if any)
        path_csv_test = ''
        with open(file, 'r') as datfile:
            lines = datfile.readlines()
            for i,line in enumerate(lines):
                if '- Target value:' in line:
                    y_value = line.split()[-1]
                elif '- Names:' in line:
                    names = line.split()[-1]
                elif 'External set with predicted results:' in line:
                    path_csv_test = line.split()[-1]
                    csv_test_exists = True
        
        if csv_test_exists:
            html_predictions = ''
            # add metrics
            columns_metrics = []
            for suffix in ['No PFI','PFI']:
                metrics_dat = get_csv_metrics(module_file,suffix)
                if metrics_dat != f'<u>csv_test metrics</u>\n':
                    columns_metrics.append(metrics_dat)
            
            # combine both metrics columns and add them if they exist
            if len(columns_metrics) == 2:
                metrics = combine_cols(columns_metrics)
                html_predictions += f'<pre style="text-align: justify;">{metrics}</pre>'

            # add csv_test images if they exist
            for _,image_pair_test in enumerate(image_pair_list_test):
                pair_list = ''.join([f'<img src="file:///{image_path}" style="margin-bottom: 10px; margin-top: 10px; margin-left: -13px; width: 100%, margin: 0"/>' for image_path in image_pair_test])
                html_predictions += f'<pre style="text-align: center;">{pair_list}</pre>'

            # add predictions table
            columns_predictions = []
            if pred_type == 'reg':
                for suffix in ['No PFI','PFI']:
                    pred_dat = get_csv_pred(suffix,path_csv_test,y_value,names)
                    columns_predictions.append(pred_dat)
                
                # Combine both prediction columns
                predictions = combine_cols(columns_predictions)
                html_predictions += f'<pre style="text-align: justify;">{predictions}</pre>'

    else:
        pair_list = ''.join([f'<img src="file:///{image_path}" style="margin-bottom: 10px; margin-top: 10px; margin-left: -13px; width: 100%, margin: 0"/>' for image_path in module_images])
        html_png = f'<pre style="text-align: center;">{pair_list}</pre>'

    imag_lines = f"""
<p style="text-align: center; margin: 0;"><span style="font-weight:bold;">
{image_caption}
</span></p>
{html_png}
"""

    if csv_test_exists:
        imag_lines += html_predictions

    return imag_lines


def get_summary(module,file,suffix,titles=True,pred_type='reg'):
    """
    Retrieve the summary of results from the PREDICT and VERIFY dat files
    """
    
    if module == 'PREDICT':
        with open(file, 'r') as datfile:
            lines = datfile.readlines()
            start_results,stop_results = 0,0
            train_outliers,valid_outliers,test_outliers = [],[],[]
            for i,line in enumerate(lines):
                if suffix == 'No PFI':
                    if 'o  Results saved in' in line and 'No_PFI.dat' in line:
                        start_results,stop_results = locate_results(i,lines)
                    elif 'o  Outlier values saved' in line and 'No_PFI.dat' in line:
                        train_outliers,valid_outliers,test_outliers = locate_outliers(i,lines)
                if suffix == 'PFI':
                    if 'o  Results saved in' in line and 'No_PFI.dat' not in line:
                        start_results,stop_results = locate_results(i,lines)
                    elif 'o  Outlier values saved' in line and 'No_PFI.dat' not in line:
                        train_outliers,valid_outliers,test_outliers = locate_outliers(i,lines)

            # add the summary of results of PREDICT
            # for the SCORE section, only results
            if not titles:
                start_results += 4
                summary = []
            else:
                summary = ['<u>Prediction metrics and descriptors</u>\n']
            for line in lines[start_results:stop_results+1]:
                if 'R2' in line:
                    line = line.replace('R2','R<sup>2</sup>')
                if titles:
                    summary.append(line[6:])

                else: # for the SCORE section, only results
                    if suffix == 'No PFI':
                        summary.append(line[8:])
                    elif suffix == 'PFI':
                        summary.append(f'   {line[8:]}')

            # add the outlier part
            if titles and pred_type == 'reg': # only add this to the PREDICT section, not to the SCORE section
                summary.append('\n<u>Outliers (max. 10 shown)</u>\n')
                summary = summary + train_outliers + valid_outliers + test_outliers

    elif module == 'VERIFY':
        summary = []
        with open(file, 'r') as datfile:
            lines = datfile.readlines()
            for i,line in enumerate(lines):
                if suffix == 'No PFI':
                    if 'o  VERIFY test values saved' in line and 'No_PFI.dat' in line:
                        start_results = i+2
                        stop_results = i+6
                if suffix == 'PFI':
                    if 'o  VERIFY test values saved' in line and 'No_PFI.dat' not in line:
                        start_results = i+2
                        stop_results = i+6
       
            for line in lines[start_results:stop_results+1]:
                if 'R2' in line:
                    line = line.replace('R2','R<sup>2</sup>')
                if titles:
                    summary.append(line[6:])

    summary = ''.join(summary)

    # Column 1: No PFI
    if titles:
        if suffix == 'No PFI':
            column = f"""
            <p><span style="font-weight:bold;">No PFI (all descriptors):</span></p>
            <pre style="text-align: justify;">{summary}</pre>
            """

        # Column 2: PFI
        elif suffix == 'PFI':
            if titles:
                column = f"""
                <p><span style="font-weight:bold;">PFI (only important descriptors):</span></p>
                <pre style="text-align: justify;">{summary}</pre>
                """
    
    else:
        column = f"""
        <pre style="text-align: justify;">{summary}</pre>
        """

    return column


def get_csv_metrics(file,suffix):
    """
    Retrieve the csv_test results from the PREDICT dat file
    """
    
    results_line = ''
    with open(file, 'r') as datfile:
        lines = datfile.readlines()
        for i,line in enumerate(lines):
            if suffix == 'No PFI':
                if 'o  Results saved in' in line and 'No_PFI.dat' in line:
                    for j in range(i,i+15):
                        if 'o  SHAP' in lines[j]:
                            break
                        elif '-  csv_test : ' in lines[j]:
                            results_line = lines[j][9:]
            if suffix == 'PFI':
                if 'o  Results saved in' in line and 'No_PFI.dat' not in line:
                    for j in range(i,i+15):
                        if 'o  SHAP' in lines[j]:
                            break
                        elif '-  csv_test : ' in lines[j]:
                            results_line = lines[j][9:]

    # start the csv_test section
    metrics_dat = f'<u>csv_test metrics</u>\n'

    # add line with model metrics (if any)
    if results_line != '':
        metrics_dat += f'<pre style="text-align: justify;">{results_line}</pre>'
        if suffix == 'No PFI':
            margin_size = -10
        elif suffix == 'PFI':
            margin_size = 0

        metrics_dat = f'<pre style="text-align: justify; margin-left: {margin_size}px;">{metrics_dat}</pre>'

    return metrics_dat


def get_csv_pred(suffix,path_csv_test,y_value,names):
    """
    Retrieve the csv_test results from the PREDICT dat file
    """
    
    pred_line = ''
    csv_test_folder = f'{os.getcwd()}/{os.path.dirname(path_csv_test)}'
    csv_test_list = glob.glob(f'{csv_test_folder}/*.csv')
    for file in csv_test_list:
        if suffix == 'No PFI':
            if '_predicted_No_PFI' in file:
                csv_test_file = file
        if suffix == 'PFI':
            if '_predicted_PFI' in file:
                csv_test_file = file

    csv_test_df = pd.read_csv(csv_test_file, encoding='utf-8')

    # start the csv_test section
    pred_line = f'<u>csv_test predictions (sorted, max. 20 shown)</u>\n\n'

    if suffix == 'No PFI':
        pred_line += f'From /PREDICT/csv_test/...No_PFI.csv'
    elif suffix == 'PFI':
        pred_line += f'From /PREDICT/csv_test/..._PFI.csv'

    pred_line += '''<style>
    th, td {
    border:0.75px solid black;
    border-collapse: collapse;
    padding: 2px;
    text-align: justify;
    }
    '''

    y_val_exist = False
    if f'{y_value}' in csv_test_df.columns:
        y_val_exist = True

    # adjust format of headers
    names_head = names
    if len(str(names_head)) > 12:
        names_head = f'{str(names_head[:9])}...'
    y_value_head = y_value
    if len(str(y_value_head)) > 12:
        y_value_head = f'{str(y_value_head[:9])}...'

    pred_line += f'''
    </style>
    <table style="width:80%">
        <tr>
            <td><strong>{names_head}</strong></td>'''
    if y_val_exist:
        pred_line += f'''
            <td><strong>{y_value_head}</strong></td>'''
    pred_line += f'''
            <td><strong>{y_value_head}_pred ± sd</strong></td>
        </tr>'''
    
    # retrieve and sort the values
    if not y_val_exist:
        csv_test_df[y_value] = csv_test_df[f'{y_value}_pred']
    y_pred_sorted, y_sorted, names_sorted, sd_sorted = (list(t) for t in zip(*sorted(zip(csv_test_df[f'{y_value}_pred'], csv_test_df[y_value], csv_test_df[names], csv_test_df[f'{y_value}_pred_sd']), reverse=True)))

    max_table = False
    if len(y_pred_sorted) > 20:
        max_table = True

    count_entries = 0
    for y_val_pred, y_val, name, sd in zip(y_pred_sorted, y_sorted, names_sorted, sd_sorted):
        # adjust format of entries
        if len(str(name)) > 12:
            name = f'{str(name[:9])}...'
        y_val_pred = round(y_val_pred, 2)
        y_val = round(y_val, 2)
        sd = round(sd, 2)
        y_val_pred_formatted = f'{y_val_pred} ± {sd}'
        add_entry = True
        # if there are more than 20 predictions, only 20 values will be shown
        if max_table and count_entries >= 10:
            add_entry = False
            if count_entries == 10:
                pred_line += f'''
                <tr>
                    <td>...</td>'''
                if y_val_exist:
                    pred_line += f'''
                    <td>...</td>'''
                pred_line += f'''
                    <td>...</td>
                </tr>'''
            elif count_entries >= (len(y_pred_sorted) - 10):
                add_entry = True
        if add_entry:
            pred_line += f'''
            <tr>
                <td>{name}</td>'''
            if y_val_exist:
                pred_line += f'''
                <td>{y_val}</td>'''
            pred_line += f'''
                <td>{y_val_pred_formatted}</td>
            </tr>'''
        count_entries += 1

    pred_line += f'''
    </table>'''

    if pred_line != '':
        if suffix == 'No PFI':
            margin_size = -10
        elif suffix == 'PFI':
            margin_size = 0
        prediction_dat = f'<pre style="text-align: justify; margin-left: {margin_size}px;">{pred_line}</pre>'

    return prediction_dat


def locate_outliers(i,lines):
    """
    Returns the start and end of the PREDICT summary in the dat file
    """
    
    train_outliers,valid_outliers,test_outliers = [],[],[]
    len_line = 54
    for j in range(i+1,len(lines)):
        if 'Train:' in lines[j]:
            for k in range(j,len(lines)):
                if 'Validation:' in lines[k]:
                    break
                elif len(train_outliers) <= 10: # 10 outliers and the line with the % of outliers
                    if len(lines[k][6:]) > len_line:
                        outlier_line = f'{lines[k][6:len_line+6]}\n{lines[k][len_line+6:]}'
                    else:
                        outlier_line = lines[k][6:]
                    train_outliers.append(outlier_line)
        elif 'Validation:' in lines[j]:
            for k in range(j,len(lines)):
                if 'Test:' in lines[k] or len(lines[k].split()) == 0:
                    break
                elif len(valid_outliers) <= 10: # 10 outliers and the line with the % of outliers
                    if len(lines[k][6:]) > len_line:
                        outlier_line = f'{lines[k][6:len_line+6]}\n{lines[k][len_line+6:]}'
                    else:
                        outlier_line = lines[k][6:]
                    valid_outliers.append(outlier_line)
        elif 'Test:' in lines[j]:
            for k in range(j,len(lines)):
                if len(lines[k].split()) == 0:
                    break
                elif len(test_outliers) <= 10: # 10 outliers and the line with the % of outliers
                    if len(lines[k][6:]) > len_line:
                        outlier_line = f'{lines[k][6:len_line+6]}\n{lines[k][len_line+6:]}'
                    else:
                        outlier_line = lines[k][6:]
                    test_outliers.append(outlier_line)

        if len(lines[j].split()) == 0:
            break

    return train_outliers,valid_outliers,test_outliers


def locate_results(i,lines):
    """
    Returns the start and end of the outliers section in the PREDICT dat file
    """
    
    start_results = i+1
    stop_results = i+6
    for j in range(i+1,i+10):
        if '-  Test : ' in lines[j]:
            stop_results = i+7
    
    return start_results,stop_results

   
def combine_cols(columns):
    """
    Makes a string with multi-column lines
    """
    
    column_data = ''
    for column in columns:
        column_data += f'<div style="flex: 1;">{column}</div>'

    combined_data = f"""
    <div style="display: flex;">
    {column_data}
    </div>
    """

    return combined_data


def revert_list(list_tuple):
    """
    Reverts the order of a list of two components
    """

    new_sort = [] # for some reason reverse() gives a weird issue when reverting lists
    new_sort.append(list_tuple[1])
    new_sort.append(list_tuple[0])

    return new_sort


def get_time(file):
    """
    Returns the execution time of the modules
    """

    module_time = 'x  ROBERT did not save any execution time for this module'
    with open(file, 'r') as datfile:
        for line in reversed(datfile.readlines()):
            if 'Time' in line and 'seconds' in line:
                module_time = line
                break

    return module_time


def get_col_score(score_info,data_score,suffix,csv_test,spacing_PFI,pred_type,test_set):
    """
    Gather the information regarding the score of the No PFI and PFI models
    """
    
    r2_pts = get_pts(data_score['r2_score'])
    outliers_pts = get_pts(data_score['outliers_score'])
    descp_pts = get_pts(data_score['descp_score'])
    verify_pts = get_pts(data_score['verify_score'])
    verify_extra_pts = get_pts(data_score['verify_extra_score'])

    spacing_r2 = get_spacing(data_score['r2_score'])
    spacing_outliers = get_spacing(data_score['outliers_score'])
    spacing_descp = get_spacing(data_score['descp_score'])
    spacing_verify = get_spacing(data_score['verify_score'])
    spacing_extra_verify = get_spacing(data_score['verify_extra_score'])

    first_line = f'<p style="text-align: justify; margin-top: -8px;">{spacing_PFI}' # reduces line separation separation
    reduced_line = f'<p style="text-align: justify; margin-top: -10px;">{spacing_PFI}' # reduces line separation separation        

    if test_set:
        score_set = 'test'
    else:
        score_set = 'valid.'
        
    ML_line_format = f'<p style="text-align: justify; margin-top: -2px; margin-bottom: 0px;">{spacing_PFI}'
    part_line_format = f'<p style="text-align: justify; margin-top: 0px; margin-bottom: 4px;">{spacing_PFI}'

    if suffix == 'No PFI':
        caption = f'{spacing_PFI}No PFI (all descriptors):'

    elif suffix == 'PFI':
        caption = f'{spacing_PFI}PFI (only important descriptors):'

    if pred_type == 'reg':
        score_info += f"""{first_line}{r2_pts}{spacing_r2} The {score_set} set shows an R<sup>2</sup> of {data_score['r2_valid']}</p>
{reduced_line}{outliers_pts}{spacing_outliers} The valid. set has {data_score['outliers_prop']}% of outliers</p>
{reduced_line}{descp_pts}{spacing_descp} Using {data_score['proportion_ratio']} points(train+valid.):descriptors</p>
{reduced_line}{verify_pts}{spacing_verify} The valid. set passes {data_score['verify_score']} VERIFY tests</p>
"""

    elif pred_type == 'clas':
        score_info += f"""{first_line}{r2_pts}{spacing_r2} The {score_set} set shows an MCC of {data_score['mcc_valid']}</p>
{reduced_line}{descp_pts}{spacing_descp} The {score_set} set uses {data_score['proportion_ratio']} points:descriptors</p>
{reduced_line}{verify_pts}{spacing_verify} The {score_set} set passes {data_score['verify_score']} VERIFY tests</p>
{reduced_line}{verify_extra_pts}{spacing_extra_verify} The {score_set} set passes {data_score['verify_extra_score']} y-mean/y-shuffle</p>
"""
    partitions_ratio = data_score['proportion_ratio_print'].split('-  ')[1]
    
    column = f"""<p style="margin-top:-12px;"><span style="font-weight:bold;">{caption}</span></p>
    {ML_line_format}ML model: {data_score['ML_model']}</p>
    {part_line_format}{partitions_ratio}</p>
    {score_info}
    <p style="margin-bottom: 20px;"></p>
    """

    return column


def get_col_text(type_thres):
    """
    Gather the information regarding the thresholds used in the score and abbreviation sections
    """

    reduced_line = '<p style="text-align: justify; margin-top: -8px;">' # reduces line separation separation

    first_line = '<p style="text-align: justify; margin-top: -25px;">'
    if type_thres == 'abbrev_1':
        abbrev_list = ['<strong>ACC:</strong> accuracy',
                        '<strong>ADAB:</strong> AdaBoost',
                        '<strong>CSV:</strong> comma separated values',
                        '<strong>CLAS:</strong> classification',
                        '<strong>CV:</strong> cross-validation',
                        '<strong>F1 score:</strong> balanced F-score',
                        '<strong>GB:</strong> gradient boosting',
                        '<strong>GP:</strong> gaussian process'
        ]

    elif type_thres == 'abbrev_2':
        abbrev_list = ['<strong>KN:</strong> k-nearest neighbors',
                        '<strong>MAE:</strong> root-mean-square error', 
                        "<strong>MCC:</strong> Matthew's correl. coefficient",
                        '<strong>ML:</strong> machine learning',                          
                        '<strong>MVL:</strong> multivariate lineal models',
                        '<strong>NN:</strong> neural network',
                        '<strong>PFI:</strong> permutation feature importance',
                        '<strong>R2:</strong> coefficient of determination'
        ]
    elif type_thres == 'abbrev_3':
        abbrev_list = ['<strong>REG:</strong> Regression',
                        '<strong>RF:</strong> random forest',
                        '<strong>RMSE:</strong> root mean square error',
                        '<strong>RND:</strong> random',
                        '<strong>SHAP:</strong> Shapley additive explanations',
                        '<strong>VR:</strong> voting regressor',
        ]

    column = ''
    for i,ele in enumerate(abbrev_list):
        if i == 0:
            column += f"""{first_line}{ele}</p>
"""
        else:
            column += f"""{reduced_line}{ele}</p>
"""

    return column


def get_col_transpa(params_dict,suffix,section):
    """
    Gather the information regarding the model parameters represented in the Reproducibility section
    """

    first_line = '<p style="text-align: justify; margin-top: -40px;">' # reduces line separation separation
    reduced_line = '<p style="text-align: justify; margin-top: -8px;">' # reduces line separation separation

    if suffix == 'No_PFI':
        caption = f'No PFI (all descriptors):'

    elif suffix == 'PFI':
        caption = f'PFI (only important descriptors):'

    excluded_params = [params_dict['error_type'],'train','y']
    misc_params = ['split','type','error_type']
    if params_dict['type'] == 'reg':
        model_type = 'Regressor'
    elif params_dict['type'] == 'clas':
        model_type = 'Classifier'
    models_dict = {'RF': f'RandomForest{model_type}',
                    'MVL': 'LinearRegression',
                    'GB': f'GradientBoosting{model_type}',
                    'NN': f'MLP{model_type}',
                    'GP': f'GaussianProcess{model_type}',
                    'ADAB': f'AdaBoost{model_type}',
                    'VR': f'Voting{model_type} (combining RF, GB and NN)'
                    }

    col_info,sklearn_model = '',''
    for _,ele in enumerate(params_dict.keys()):
        if ele not in excluded_params:
            if ele == 'model' and section == 'model_section':
                sklearn_model = models_dict[params_dict[ele].upper()]
                sklearn_model = f"""{first_line}sklearn model: {sklearn_model}</p>"""
            elif section == 'model_section' and ele.lower() not in misc_params:
                if ele != 'X_descriptors':
                    if ele == 'seed':
                        col_info += f"""{reduced_line}random_state: {params_dict[ele]}</p>"""
                    else:
                        col_info += f"""{reduced_line}{ele}: {params_dict[ele]}</p>"""
            elif section == 'misc_section' and ele.lower() in misc_params:
                if col_info == '':
                    col_info += f"""{first_line}{ele}: {params_dict[ele]}</p>"""
                else:
                    col_info += f"""{reduced_line}{ele}: {params_dict[ele]}</p>"""
    
    column = f"""<p style="margin-top: -30px;"><span style="font-weight:bold;">{caption}</span></p>
    {sklearn_model}{col_info}
    """

    return column


def get_pts(score):
    """
    Get a string with the numbers of points or lines to add
    """

    if score > 0:
        str_pts = f"{score*'&#9679;'}"
    else:
        str_pts = f"-"

    return str_pts


def get_spacing(score):
    """
    Get a string with the number of spaces needed
    """

    if score == 4:
        spacing = f"&nbsp;"
    if score == 3:
        spacing = f"&nbsp;&nbsp;&nbsp;"
    if score == 2:
        spacing = f"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"
    if score == 1:
        spacing = f"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"
    if score == 0:
        spacing = f"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"

    return spacing


def get_verify_scores(dat_verify,suffix,pred_type):
    """
    Calculates scores that come from the VERIFY module (VERIFY tests)
    """

    start_data = False
    verify_score, verify_extra_score = 0,0
    for i,line in enumerate(dat_verify):
        # set starting points for No PFI and PFI models
        if suffix == 'No PFI':
            if '------- ' in line and '(No PFI)' in line:
                start_data = True
            elif '------- ' in line and 'with PFI' in line:
                start_data = False
        if suffix == 'PFI':
            if '------- ' in line and 'with PFI' in line:
                start_data = True
        
        if start_data:
            if 'Original ' in line and '(valid' in line:
                for j in range(i+1,i+5):
                    if 'PASSED' in dat_verify[j]:
                        verify_score += 1
                if pred_type == 'clas':
                    if 'PASSED' in dat_verify[i+2]:
                        verify_extra_score += 1
                    if 'PASSED' in dat_verify[i+3]:
                        verify_extra_score += 1

    return verify_score,verify_extra_score


def get_predict_scores(dat_predict,suffix,pred_type):
    """
    Calculates scores that come from the PREDICT module (R2 or accuracy, datapoints:descriptors ratio, outlier proportion)
    """

    start_data, test_set = False, False
    data_score = {}
    data_score['r2_score'] = 0
    data_score['descp_score'] = 0
    data_score['outliers_score'] = 0

    for i,line in enumerate(dat_predict):

        # set starting points for No PFI and PFI models
        if suffix == 'No PFI':
            if '------- ' in line and '(No PFI)' in line:
                start_data = True
            elif '------- ' in line and 'with PFI' in line:
                start_data = False
        if suffix == 'PFI':
            if '------- ' in line and 'with PFI' in line:
                start_data = True
        
        if start_data:
            # R2 and proportion
            if 'o  Results saved in PREDICT/' in line:
                data_score['ML_model'] = line.split('_')[1]
                data_score['proportion_ratio_print'] = dat_predict[i+2]
                # R2/accuracy from test (if any) or validation
                if pred_type == 'reg':
                    if '-  Test : R2' in dat_predict[i+7]:
                        data_score['r2_valid'] = float(dat_predict[i+7].split()[5].split(',')[0])
                        test_set = True
                    elif '-  Valid. : R2' in dat_predict[i+6]:
                        data_score['r2_valid'] = float(dat_predict[i+6].split()[5].split(',')[0])
                   
                    if data_score['r2_valid'] > 0.85:
                        data_score['r2_score'] += 2
                    elif data_score['r2_valid'] > 0.7:
                        data_score['r2_score'] += 1

                elif pred_type == 'clas':
                    if '-  Test : Accuracy' in dat_predict[i+7]:
                        data_score['mcc_valid'] = float(dat_predict[i+7].split()[-1])
                        test_set = True
                    elif '-  Valid. : Accuracy' in dat_predict[i+6]:
                        data_score['mcc_valid'] = float(dat_predict[i+6].split()[-1])

                    if data_score['mcc_valid'] > 0.85:
                        data_score['r2_score'] += 2
                    elif data_score['mcc_valid'] > 0.7:
                        data_score['r2_score'] += 1

                # proportion
                data_score['proportion_ratio'] = dat_predict[i+4].split()[-1]
                proportion = int(data_score['proportion_ratio'].split(':')[0]) / int(data_score['proportion_ratio'].split(':')[1])
                if proportion > 10:
                    data_score['descp_score'] += 2
                elif proportion > 3:
                    data_score['descp_score'] += 1

            # outliers
            if pred_type == 'reg':
                if 'o  Outlier values saved in' in line:
                    for j in range(i,len(dat_predict)):
                        if 'Validation:' in dat_predict[j]:
                            outliers_prop = dat_predict[j].split()[-1]
                            outliers_prop = outliers_prop.split('%)')[0]
                            data_score['outliers_prop'] = float(outliers_prop.split('(')[-1])
                        elif len(dat_predict[j].split()) == 0:
                            break
                    if data_score['outliers_prop'] < 7.5:
                        data_score['outliers_score'] += 2
                    elif data_score['outliers_prop'] < 15:
                        data_score['outliers_score'] += 1

            elif pred_type == 'clas':
                data_score['outliers_prop'] = 0

    return data_score,test_set


def repro_info(modules):
    """
    Retrieves variables used in the Reproducibility and Transparency section
    """

    version_n_date, citation, command_line = '','',''
    python_version, intelex_version, total_time = '','',0
    intelex_installed = True
    dat_files = {}
    for module in modules:
        path_file = Path(f'{os.getcwd()}/{module}/{module}_data.dat')
        if os.path.exists(path_file):
            datfile = open(path_file, 'r', errors="replace")
            txt_file = []
            for line in datfile:
                txt_file.append(line)
                if module.upper() in ['GENERATE','VERIFY','PREDICT']:
                    if 'The scikit-learn-intelex accelerator is not installed' in line:
                        intelex_installed = False
                if 'Time' in line and 'seconds' in line:
                    total_time += float(line.split()[2])
                if 'How to cite: ' in line:
                    citation = line.split('How to cite: ')[1]
                if 'ROBERT v' == line[:8]:
                    version_n_date = line
                if 'Command line used in ROBERT: ' in line:
                    command_line = line.split('Command line used in ROBERT: ')[1]
            total_time = round(total_time,2)
            dat_files[module] = txt_file
            datfile.close()
 
    try:
        import platform
        python_version = platform.python_version()
    except:
        python_version = '(version could not be determined)'
    if intelex_installed:
        try:
            import pkg_resources
            intelex_version = pkg_resources.get_distribution("scikit-learn-intelex").version
        except:
            intelex_version = '(version could not be determined)'
    else:
        intelex_version = 'not installed'
    
    return version_n_date, citation, command_line, python_version, intelex_version, total_time, dat_files


def make_report(report_html, HTML):
    """
    Generate a css file that will be used to make the PDF file
    """

    css_files = ["report.css"]
    outfile = f"{os.getcwd()}/ROBERT_report.pdf"
    if os.path.exists(outfile):
        try:
            os.remove(outfile)
        except PermissionError:
            print('\nx  ROBERT_report.pdf is open! Please, close the PDF file and run ROBERT again with --report (i.e., "python -m robert --report").')
            sys.exit()
    pdf = make_pdf(report_html, HTML, css_files)
    _ = Path(outfile).write_bytes(pdf)


def make_pdf(html, HTML, css_files):
    """Generate a PDF file from a string of HTML"""
    htmldoc = HTML(string=html, base_url="")
    if css_files:
        htmldoc = htmldoc.write_pdf(stylesheets=css_files)
    else:
        htmldoc = htmldoc.write_pdf()
    return htmldoc


def css_content(csv_name,robert_version):
    """
    Obtain ROBERT version and CSV name to use it on top of the PDF report
    """

    css_content = f"""
    body {{
    font-size: 12px;
    line-height: 1.5;
    }}
    @page {{
        size: A4;
        margin: 2cm;
        @bottom-right {{
            content: "Page "counter(page) " of " counter(pages);
            font-size: 8pt;
            position: fixed;
            right: 0;
            bottom: 0;
            transform: translateY(-8pt);
            white-space: nowrap;
        }}  
        @bottom-left {{
            content: "ROBERT v {robert_version}";
            font-size: 8pt;
            position: fixed;
            left: 0;
            bottom: 0;
            transform: translateY(-8pt);
            white-space: nowrap;
        }}
        @bottom-center {{
            content: "";
            border-top: 3px solid black;
            width: 100%;
            position: fixed;
            left: 0;
            right: 0;
            bottom: 0pt;
            transform: translateY(8pt);
        }}  
        @top-center {{
            content: "";
            border-top: 3px solid black;
            width: 100%;
            position: fixed;
            left: 0;
            right: 0;
            bottom: 0pt;
            transform: translateY(40pt);
        }}  
        @top-left {{
            content: "ROBERT Report";
            font-size: 8pt;
            font-weight:bold;
            position: fixed;
            position: fixed;
            left: 0;
            right: 0;
            bottom: 0pt;
            transform: translateY(2pt);
            white-space: nowrap;
        }} 
        @top-right {{
            content: "{csv_name}";
            font-size: 8pt;
            font-style: italic;
            position: fixed;
            position: fixed;
            left: 0;
            right: 0;
            bottom: 0pt;
            transform: translateY(2pt);
            white-space: nowrap;
        }} 
    }}
    * {{
        font-family: "Helvetica", Arial, sans-serif;
    }}
    .dat-content {{
        width: 50%;
        max-width: 595pt;
        overflow-x: auto;
        line-height: 1.2;
    }}

    img[src="Robert_logo.jpg"] {{
        float: center;
    }}
    img[src*="Pearson"] {{
        display: inline-block;
        vertical-align: bottom;
        max-width: 48%;
        margin-left: 10px;
        margin-bottom: -5px;
    }}
    img[src*="PFI"] {{
        display: inline-block;
        vertical-align: bottom;
        max-width: 48%;
        margin-left: 10px;
        margin-bottom: -5px;
    }}

    img[src*="PFI"]:first-child {{
        margin-right: 10%;
    }}
    .img-PREDICT {{
        margin-top: 20px;
    }}
    
    hr.black {{
    border: none;
    height: 3px;
    background-color: black;
    }}
    
    hr {{
    border: none;
    height: 1px;
    background-color: gray;
    }}

    body:before {{
    top: 1.2cm;
    }}
    """
    return css_content


def format_lines(module_data, max_width=122, cmd_line=False):
    """
    Reads a file and returns a formatted string between two markers
    """

    formatted_lines = []
    lines = module_data.split('\n')
    for i,line in enumerate(lines):
        if 'R2' in line:
            line = line.replace('R2','R<sup>2</sup>')
        if cmd_line:
            formatted_line = textwrap.fill(line, width=max_width-5, subsequent_indent='')
        else:
            formatted_line = textwrap.fill(line, width=max_width, subsequent_indent='')
        if i > 0:
            formatted_lines.append(f'<pre style="text-align: justify;">\n{formatted_line}</pre>')
        else:
            formatted_lines.append(f'<pre style="text-align: justify;">{formatted_line}</pre>\n')

    return ''.join(formatted_lines)


def get_y_values(file,y_param):
    """
    Gets y and y_pred values from the PREDICT folder
    """

    df = pd.read_csv(file, encoding='utf-8')

    y_val = df[y_param]
    y_val_pred = df[f'{y_param}_pred']

    return y_val,y_val_pred
