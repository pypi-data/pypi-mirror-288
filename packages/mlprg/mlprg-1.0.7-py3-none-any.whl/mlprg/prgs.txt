# Program 1
import csv
def loadcsv(filename):
  lines=csv.reader(open(filename,"r"))
  dataset = list(lines)
  headers=dataset.pop(0)
  return dataset,headers
def print_hypoyhesis(h):
  print('<',end='')
  for i in range(0,len(h)-1):
    print(h[i],end=',')
  print('>')
def findS():
  dataset,features=loadcsv("sports1.csv")
  rows=len(dataset)
  cols=len(dataset[0])
  flag=0
  for x in range(0,rows):
    t=dataset[x]
    if t[-1]=='1' and flag==0:
      flag=1
      h=dataset[x]
    elif t[-1] == '1':
      for y in range(cols):
        if h[y]!=t[y]:
          h[y]='?'
  print("The maximally specific hypothessis for a given trainsing exams")
  print_hypoyhesis(h)
findS()

# Program 2

import numpy as np
import pandas as pd

df = pd.read_csv("sports1.csv")
concept=np.array(df.iloc[:,0:-1])
target=np.array(df.iloc[:,-1])

def learn(concept,target):
    specific_h=concept[0].copy()
    print("Most specific",specific_h)
    general_h=[["?" for i in range(len(specific_h))] for i in range(len(specific_h))]
    print("General",general_h)

    for i,h in enumerate(concept):
        print("Instances", i+1, "is", h)
        if target[i]==1:
            print("Instance is positive")
            for x in range(len(specific_h)):
                if(h[x]!=specific_h[x]):
                    specific_h[x]='?'
                    general_h[x][x]='?'

        if(target[i]==0):
            print("instance is negative")
            for x in range(len(specific_h)):
                if h[x]!=specific_h[x]:
                    general_h[x][x]=specific_h[x]
                else:
                    general_h[x][x]='?'
        print("Specific boundary", i+1, specific_h)
        print("General bundary", i+1, general_h)
        print("\n")

learn(concept, target)

# Program 3

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt
import warnings
warnings.filterwarnings('ignore')
from sklearn import tree

df=pd.read_csv("Iris.csv")
df.head()

df.drop('Id',axis=1,inplace=True)
df.head()

le = LabelEncoder()
df['Species']= le.fit_transform(df['Species'])
df['Species'].unique()

X=df.iloc[:,:4]
y=df.iloc[:,4:]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=123)

clf=DecisionTreeClassifier(criterion='entropy', splitter='best', max_leaf_nodes=3)
clf.fit(X_train,y_train.values.ravel())
y_pred=clf.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

confusion_matrix(y_test, y_pred)

fn=['SepalLengthCm' ,'SepalWidthCm' ,'PetalLengthCm',   'PetalWidthCm']#column names of the dataset
cn=['Iris-setosa',' Iris-versicolor','Iris-virginica']#names of classes to be classified

fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (2,2), dpi=200)
tree.plot_tree(clf,
               feature_names = fn,
               class_names=cn,
               filled = True);

species_check = clf.predict([[4.7,  3.2,    1.3,    0.2]])[0]

# Program 4

import numpy as np
from sklearn.neural_network import MLPRegressor

# Data normalization
X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)
X= X/ np.amax(X, axis=0)
y = np.array(([92], [86], [89]), dtype=float) / 100

# Define and train the neural network
model = MLPRegressor(hidden_layer_sizes=(3,), activation='logistic', max_iter=5000, learning_rate_init=0.1, solver='sgd')
model.fit(X, y.ravel())

# Predictions
predicted_output = model.predict(X)

print("Input:\n", X)
print("Actual Output:\n", y)
print("Predicted Output:\n", predicted_output.reshape(-1, 1))

# Program 5

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.metrics import classification_report

df=pd.read_csv('naivebasedataset.csv',header=None)
X=df.drop([8],axis=1)
Y=df[8]
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=1)

gn=GaussianNB()
gn.fit(x_train,y_train)
y_pred=gn.predict(x_test)

print(len(x_test))
print(len(x_train))
print(metrics.accuracy_score(y_test,y_pred)*100)
print(classification_report(y_test,y_pred))

# Program 5l

import csv, random, math
import statistics as st

def loadCsv(filename):
    lines = csv.reader(open(filename, "r"));
    dataset = list(lines)
    for i in range(len(dataset)):
        dataset[i] = [float(x) for x in dataset[i]]
    return dataset

def splitDataset(dataset, splitRatio):
    testSize = int(len(dataset) * splitRatio);
    trainSet = list(dataset);
    testSet = []
    while len(testSet) < testSize:
        index = random.randrange(len(trainSet));
        testSet.append(trainSet.pop(index))
    return [trainSet, testSet]
def separateByClass(dataset):
    separated = {}
    for i in range(len(dataset)):
        x = dataset[i]
        if (x[-1] not in separated):
            separated[x[-1]] = []
        separated[x[-1]].append(x)
    return separated
def compute_mean_std(dataset):
    mean_std = [ (st.mean(attribute), st.stdev(attribute))
                for attribute in zip(*dataset)];
    del mean_std[-1]
    return mean_std
def summarizeByClass(dataset):
    separated = separateByClass(dataset);
    summary = {}
    for classValue, instances in separated.items():
        summary[classValue] = compute_mean_std(instances)
    return summary
def estimateProbability(x, mean, stdev):
    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))
    return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent
def calculateClassProbabilities(summaries, testVector):
    p = {}
    for classValue, classSummaries in summaries.items():
        p[classValue] = 1
        for i in range(len(classSummaries)):
            mean, stdev = classSummaries[i]
            x = testVector[i]
            p[classValue] *= estimateProbability(x, mean, stdev);
    return p
def predict(summaries, testVector):
    all_p = calculateClassProbabilities(summaries, testVector)
    bestLabel, bestProb = None, -1
    for lbl, p in all_p.items():
        if bestLabel is None or p > bestProb:
            bestProb = p
            bestLabel = lbl
    return bestLabel
def perform_classification(summaries, testSet):
    predictions = []
    for i in range(len(testSet)):
        result = predict(summaries, testSet[i])
        predictions.append(result)
    return predictions
def getAccuracy(testSet, predictions):
    correct = 0
    for i in range(len(testSet)):
        if testSet[i][-1] == predictions[i]:
            correct += 1
    return (correct/float(len(testSet))) * 100.0
dataset = loadCsv('naivebasedataset.csv');
print('Pima Indian Diabetes Dataset loaded...')
print('Total instances available :',len(dataset))
print('Total attributes present :',len(dataset[0])-1)
print("First Five instances of dataset:")
for i in range(5):
    print(i+1 , ':' , dataset[i])
splitRatio = 0.2
trainingSet, testSet = splitDataset(dataset, splitRatio)
print('\nDataset is split into training and testing set.')
print('Training examples = {0} \nTesting examples = {1}'.format(len(trainingSet),
len(testSet)))
summaries = summarizeByClass(trainingSet)
predictions = perform_classification(summaries, testSet)
accuracy = getAccuracy(testSet, predictions)
print('\nAccuracy of the Naive Baysian Classifier is :', accuracy)


# Program 6

import matplotlib.pyplot as plt
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import pandas as pd

df = pd.read_csv("Iris.csv")
df.drop('Id',axis=1,inplace=True)
df.head()

df.info()

le = LabelEncoder()
df['Species']= le.fit_transform(df['Species'])
df['Species'].unique()

X=df.iloc[:,:4]
y=df.iloc[:,4:]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=123)

svm = SVC(kernel="rbf", gamma=0.5, C=1.0)
svm.fit(X_train, y_train)
y_prediction=svm.predict(X_test)

class_names=["Iris-setosa","Iris-virginica","Iris-versicolor"]
print(classification_report(y_test, y_prediction ,target_names=class_names))


x=df.iloc[:,:2]
svm2 = SVC(kernel="rbf", gamma=0.5, C=1.0)
svm2.fit(x,y)
DecisionBoundaryDisplay.from_estimator(
		svm2,
		x,
		response_method="predict",
		cmap=plt.cm.Spectral,
		alpha=0.8,
	)


# Program 7

import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.cluster import KMeans
import sklearn.metrics as sm
import pandas as pd
import numpy as np

iris = datasets.load_iris()

X = pd.DataFrame(iris.data)
X.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']

y = pd.DataFrame(iris.target)
y.columns = ['Targets']

model = KMeans(n_clusters=3)
model.fit(X)


plt.figure(figsize=(14,7))

colormap = np.array(['red', 'lime', 'black'])

# Plot the Original Classifications
plt.subplot(1, 2, 1)
plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y.Targets], s=40)
plt.title('Real Classification')
plt.xlabel('Petal Length')
plt.ylabel('Petal Width')


# Plot the Models Classifications
plt.subplot(1, 2, 2)
plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[model.labels_], s=40)
plt.title('K Mean Classification')
plt.xlabel('Petal Length')
plt.ylabel('Petal Width')
print('The accuracy score of K-Mean: ',sm.accuracy_score(y, model.labels_))
print('The Confusion matrixof K-Mean: ',sm.confusion_matrix(y, model.labels_))


from sklearn import preprocessing
scaler = preprocessing.StandardScaler()
scaler.fit(X)
xsa = scaler.transform(X)
xs = pd.DataFrame(xsa, columns = X.columns)
#xs.sample(5)

from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=3)
gmm.fit(xs)

y_gmm = gmm.predict(xs)
#y_cluster_gmm

plt.subplot(2, 2, 3)
plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y_gmm], s=40)
plt.title('GMM Classification')
plt.xlabel('Petal Length')
plt.ylabel('Petal Width')

print('The accuracy score of EM: ',sm.accuracy_score(y, y_gmm))
print('The Confusion matrix of EM: ',sm.confusion_matrix(y, y_gmm))

# Program 8

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import datasets

iris=datasets.load_iris()

x = iris.data
y = iris.target

print ('sepal-length', 'sepal-width', 'petal-length', 'petal-width')
print(x)
print('class: 0-Iris-Setosa, 1- Iris-Versicolour, 2- Iris-Virginica')
print(y)

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)

#To Training the model and Nearest nighbors K=5
classifier = KNeighborsClassifier(n_neighbors=5)
classifier.fit(x_train, y_train)

#To make predictions on our test data
y_pred=classifier.predict(x_test)

print('Confusion Matrix')
print(confusion_matrix(y_test,y_pred))
print('Accuracy Metrics')
print(classification_report(y_test,y_pred))

# Program 9

import numpy as np
from bokeh.plotting import figure, show, output_notebook
from bokeh.layouts import gridplot


output_notebook()

def local_regression(x0, X, Y, tau):

    x0 = np.r_[1, x0]  
    X = np.c_[np.ones(len(X)), X]

    xw = X.T * radial_kernel(x0, X, tau)  

    beta = np.linalg.pinv(xw @ X) @ xw @ Y 

    
    return x0 @ beta  

def radial_kernel(x0, X, tau):
   
    return np.exp(np.sum((X - x0) ** 2, axis=1) / (-2 * tau * tau))

n = 1000
X = np.linspace(-3, 3, num=n)
print("The Data Set (10 Samples) X :\n", X[:10])
Y = np.log(np.abs(X ** 2 - 1) + .5)
print("The Fitting Curve Data Set (10 Samples) Y :\n", Y[:10])


X += np.random.normal(scale=.1, size=n)
print("Normalized (10 Samples) X :\n", X[:10])


domain = np.linspace(-3, 3, num=300)
print("X0 Domain Space (10 Samples) :\n", domain[:10])

def plot_lwr(tau):
    
    prediction = [local_regression(x0, X, Y, tau) for x0 in domain]
    plot = figure(width=400, height=400)
    plot.title.text = f'tau={tau}'
    plot.scatter(X, Y, alpha=.3)
    plot.line(domain, prediction, line_width=2, color='red')
    return plot


plots = [
    [plot_lwr(10.), plot_lwr(1.)],
    [plot_lwr(0.1), plot_lwr(0.01)]
]
show(gridplot(plots))

# Program 10

!pip install pgmpy

import pandas as pd
import numpy as np
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.models import BayesianModel
from pgmpy.inference import VariableElimination
import networkx as nx
import matplotlib.pyplot as plt

df=pd.read_csv("medical dataset.csv")
df.head()

print(df.info())
print(pd.unique(df['age']))

model= BayesianModel([('age','heartdisease'),('gender','heartdisease'),('exang','heartdisease'),('cp','heartdisease'),('heartdisease','restecg'),('heartdisease','chol')])

model.fit(df,estimator=MaximumLikelihoodEstimator)

print('\n Inferencing with Bayesian Network:')
HeartDisease_infer = VariableElimination(model)

print('\n 1. Probability of HeartDisease given evidence= cp')
q1=HeartDisease_infer.query(variables=['heartdisease'],evidence={'gender':1})
print(q1)

print('\n 2. Probability of HeartDisease given evidence= restecg')
q2=HeartDisease_infer.query(variables=['heartdisease'],evidence={'restecg':1})
print(q2)

graph = nx.DiGraph(model.edges())
nx.draw_networkx(graph,with_labels=True)
#plot
plt.show()

